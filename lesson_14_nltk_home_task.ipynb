{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "# Natural Language Toolkit (NLTK)\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## 1. NLTK Corpora\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download() - just once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to /home/roma/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/treebank.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download('treebank')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### 1.1. NLTK tokenized texts \n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Text: Moby Dick by Herman Melville 1851>\n"
     ]
    }
   ],
   "source": [
    "print (text1) \n",
    "# type(text7) # nltk.text.Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "texts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent1: Call me Ishmael .\n",
      "sent2: The family of Dashwood had long been settled in Sussex .\n",
      "sent3: In the beginning God created the heaven and the earth .\n",
      "sent4: Fellow - Citizens of the Senate and of the House of Representatives :\n",
      "sent5: I have a problem with people PMing me to lol JOIN\n",
      "sent6: SCENE 1 : [ wind ] [ clop clop clop ] KING ARTHUR : Whoa there !\n",
      "sent7: Pierre Vinken , 61 years old , will join the board as a nonexecutive director Nov. 29 .\n",
      "sent8: 25 SEXY MALE , seeks attrac older single lady , for discreet encounters .\n",
      "sent9: THE suburb of Saffron Park lay on the sunset side of London , as red and ragged as a cloud of sunset .\n"
     ]
    }
   ],
   "source": [
    "sents() # looke like randomly select one sentence from one text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Simple tasks with texts\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'Moby', 'Dick', 'by', 'Herman', 'Melville', '1851', ']', 'ETYMOLOGY', '.', '(', 'Supplied', 'by', 'a', 'Late', 'Consumptive', 'Usher', 'to', 'a', 'Grammar', 'School', ')', 'The', 'pale', 'Usher', '--', 'threadbare', 'in', 'coat', ',']\n"
     ]
    }
   ],
   "source": [
    "print (text1[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in text1: 260,819\n"
     ]
    }
   ],
   "source": [
    "print ('Words in text1: {:,}'.format(len(text1))) # words not chars "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words in text1: 19,317\n"
     ]
    }
   ],
   "source": [
    "print ('Unique words in text1: {:,}'.format(len(set(text1)))) # unique words not chars len(set (text1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PLINY', 'professional', 'unensanguined', 'Affected', 'baling', 'forebodings', 'ignited', 'mounted', 'selection', 'unfrequented', 'smoking', 'decanter', 'cabled', 'utmost', 'immensely', 'troughs', 'awoke', 'bless', 'demonism', 'Something']\n"
     ]
    }
   ],
   "source": [
    "print (list (set(text1))[:20]) # Note : it requires list converting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Most frequent words\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19317 <class 'nltk.probability.FreqDist'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FreqDist({',': 18713, 'the': 13721, '.': 6862, 'of': 6536, 'and': 6024, 'a': 4569, 'to': 4542, ';': 4072, 'in': 3916, 'that': 2982, ...})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Frequency distribution \n",
    "dist = FreqDist(text1) # the same as text1.vocab() \n",
    "print (len(dist) , type (dist))\n",
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[',\n",
       " 'Moby',\n",
       " 'Dick',\n",
       " 'by',\n",
       " 'Herman',\n",
       " 'Melville',\n",
       " '1851',\n",
       " ']',\n",
       " 'ETYMOLOGY',\n",
       " '.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = dist.keys()\n",
    "list (vocab)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 ['called', 'through', 'almost', 'whales', 'thought', 'before', 'against', 'towards', 'things', 'nothing', 'without', 'should', 'little', 'seemed', 'though', 'captain', 'himself', 'moment', 'CHAPTER', 'something', 'Captain', 'between', 'whaling', 'another', 'Queequeg', 'Pequod', 'Starbuck']\n"
     ]
    }
   ],
   "source": [
    "freq_words = [w for w in vocab if len(w) > 5 and dist[w] > 100] \n",
    "print (len(freq_words), freq_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's review other corpora "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### 1.2. Access the NLTK corpora\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Determining the nltk_data location\n",
    "\n",
    "</font>\n",
    "\n",
    "import nltk\n",
    "<br>\n",
    "print (nltk.\\_\\_file\\_\\_) \n",
    "<br>\n",
    "in that directory in the `data.py` you may see the path to your nltk_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK data package might reside.\n",
      "   These directories will be checked in order when looking for a\n",
      "   resource in the data package.  Note that this allows users to\n",
      "   substitute in their own versions of resources, if they have them\n",
      "   (e.g., in their home directory under ~/nltk_data).\"\"\"\n",
      "\n",
      "# \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "fp = nltk.__file__\n",
    "fp = os.path.join ('/'.join(fp.split('/')[:-1]), 'data.py')\n",
    "with open (fp , 'r') as f: \n",
    "    content = f.read()\n",
    "print (re.findall(r'NLTK data package might reside(?:.|\\s){,260}', content)[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Review NLTK corpora\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "target_directory= '/Users/new/nltk_data/corpora'\n",
    "print ( [x[0].replace(target_directory, '') for x in os.walk(target_directory)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### 1.3 Gutenberg corpus\n",
    "\n",
    "</font>\n",
    "\n",
    "NLTK includes a small selection of texts from the Project Gutenberg electronic text archive, which contains some 25,000 free electronic books, hosted at http://www.gutenberg.org/.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Access specified text \n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "[Moby Dick by Herman Melville 1851]\r\n",
      "\r\n",
      "\r\n",
      "ETYMOLOGY.\r\n",
      "\r\n",
      "(Supplied by a Late Consumptive Usher to a Grammar School)\r\n",
      "\r\n",
      "The pale Usher--threadbare in coat, heart, body, and brain; I see him\r\n",
      "now.  He was ever dusting his old lexicons and grammars, with a queer\r\n",
      "handkerchief, mockingly embellished with all the gay flags of all the\r\n",
      "known nations of the world.  He loved to dust his old grammars; it\r\n",
      "somehow mildly reminded him of his mortality.\r\n",
      "\r\n",
      "\"While you take in hand to school others, and to teac\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg \n",
    "str_moby = gutenberg.raw('melville-moby_dick.txt') \n",
    "print (type(str_moby)) # <class 'str'>\n",
    "print (str_moby[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: `str_moby` is the str whereas the `Text1` is the list of tokens derived with the help of specific pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### 1.4. Words corpus \n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'a', 'aa', 'aal', 'aalii', 'aam', 'Aani', 'aardvark', 'aardwolf', 'Aaron', 'Aaronic', 'Aaronical', 'Aaronite', 'Aaronitic', 'Aaru', 'Ab', 'aba', 'Ababdeh', 'Ababua', 'abac', 'abaca', 'abacate', 'abacay', 'abacinate', 'abacination', 'abaciscus', 'abacist', 'aback', 'abactinal', 'abactinally', 'abaction', 'abactor', 'abaculus', 'abacus', 'Abadite', 'abaff', 'abaft', 'abaisance', 'abaiser', 'abaissed', 'abalienate', 'abalienation', 'abalone', 'Abama', 'abampere', 'abandon', 'abandonable', 'abandoned', 'abandonedly', 'abandonee', 'abandoner', 'abandonment', 'Abanic', 'Abantes', 'abaptiston', 'Abarambo', 'Abaris', 'abarthrosis', 'abarticular', 'abarticulation', 'abas', 'abase', 'abased', 'abasedly', 'abasedness', 'abasement', 'abaser', 'Abasgi', 'abash', 'abashed', 'abashedly', 'abashedness', 'abashless', 'abashlessly', 'abashment', 'abasia', 'abasic', 'abask', 'Abassin', 'abastardize', 'abatable', 'abate', 'abatement', 'abater', 'abatis', 'abatised', 'abaton', 'abator', 'abattoir', 'Abatua', 'abature', 'abave', 'abaxial', 'abaxile', 'abaze', 'abb', 'Abba', 'abbacomes', 'abbacy', 'Abbadide']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import words\n",
    "correct_spellings = words.words()\n",
    "print (correct_spellings[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/roma/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## 2. Simple NLP Tasks \n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### 2.1. Tokenize\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font color = green >\n",
    "\n",
    "#### Custom Tokenizer\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['like', 'Ternopil', \"don't\", 'care', 'Ivano', \"Frankivs'k\"]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "# This was used for parcing keywords for tokens \n",
    "def tokenize_sentence(target_str):\n",
    "    target_str = re.sub(r\"[_-]\", ' ', target_str) # to get single tokens and apply the n-grams later \n",
    "    re_pattern = r\"[\\w\\']+\" \n",
    "    tokens = re.findall(re_pattern, target_str)\n",
    "    return [token for token in tokens if len(token) > 2] \n",
    "tokenize_sentence(\"I like Ternopil. I don't care of Ivano-Frankivs'k\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font color = green >\n",
    "\n",
    "#### NLTK sentence tokenizer\n",
    "\n",
    "</font>\n",
    "\n",
    "sent_tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Mr. Smith! How are you doing today? The whether is great and the Python is awesome. The sky is blue. The pen costs $2.45. What about pensil? Is ... the same price? \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Hello Mr. Smith!',\n",
       " 'How are you doing today?',\n",
       " 'The whether is great and the Python is awesome.',\n",
       " 'The sky is blue.',\n",
       " 'The pen costs $2.45.',\n",
       " 'What about pensil?',\n",
       " 'Is ... the same price?']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "target_text= '''Hello Mr. Smith! How are you doing today? The whether is great and the Python is awesome. The sky is blue. The pen costs $2.45. What about pensil? Is ... the same price? '''\n",
    "print (target_text)\n",
    "sent_tokenize(target_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### NLTK word tokenizer\n",
    "\n",
    "</font>\n",
    "\n",
    "word_tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Mr. Smith! How are you doing today? The whether is great and the Python is awesome. The sky is blue.\n",
      "['Hello', 'Mr.', 'Smith', '!', 'How', 'are', 'you', 'doing', 'today', '?', 'The', 'whether', 'is', 'great', 'and', 'the', 'Python', 'is', 'awesome', '.', 'The', 'sky', 'is', 'blue', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "target_text= '''Hello Mr. Smith! How are you doing today? The whether is great and the Python is awesome. The sky is blue.'''\n",
    "print (target_text)\n",
    "print (word_tokenize(target_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### 2.2. Stop words\n",
    "\n",
    "</font>\n",
    "\n",
    "stopwords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total len : 179\n",
      "['haven', 'before', 'should', 'won', 'this', \"that'll\", 'can', 'some', 'does', \"shan't\", 'too', 'about', 's', 'had', 'who', 'from', 'there', \"you'd\", 'the', 'our', 'that', 'into', 'being', 'he', 'needn', 'out', 'same', 'weren', 'ours', \"she's\", 'which', \"doesn't\", 'were', 'yourselves', 'an', 'so', 'further', 'where', 'of', 'not', \"didn't\", 'through', 'theirs', 'no', \"mustn't\", 'very', 'myself', 'm', 'her', 'each']\n"
     ]
    }
   ],
   "source": [
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words= set(stopwords.words('english'))\n",
    "# stop_words= set(stopwords.words('russian'))\n",
    "print ('Total len : {}'.format(len(list(stop_words))))\n",
    "print(list(stop_words)[:50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'example', 'showing', 'stop', 'words', 'filtering', '.', 'Hello', 'Mr.', 'Smith', '!', 'How', 'today', '?', 'The', 'whether', 'great', 'Python', 'awesome', '.', 'The', 'sky', 'blue', '.']\n",
      "\n",
      "Excluded from current tokens: \n",
      "['is', 'an', 'the', 'are', 'you', 'doing', 'is', 'and', 'the', 'is', 'is']\n"
     ]
    }
   ],
   "source": [
    "target_text = 'This is an example showing the stop words filtering. Hello Mr. Smith! How are you doing today? The whether is great and the Python is awesome. The sky is blue.'\n",
    "tokens = word_tokenize(target_text)\n",
    "filtered_tokens= [t for t in tokens if t not in stop_words]\n",
    "\n",
    "print (filtered_tokens)\n",
    "print ('\\nExcluded from current tokens: \\n{}'.format([t for t in tokens if t in stop_words]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### 2.3. Stemming \n",
    "\n",
    "</font>\n",
    "\n",
    "PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an example showing the stop words filtering. Hello Mr. Smith! How are you doing today? The whether is great and the Python is awesome. The sky is blue \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['thi', 'is', 'an', 'exampl', 'show', 'the', 'stop', 'word',\n",
       "       'filter', '.', 'hello', 'mr.', 'smith', '!', 'how', 'are', 'you',\n",
       "       'do', 'today', '?', 'the', 'whether', 'is', 'great', 'and', 'the',\n",
       "       'python', 'is', 'awesom', '.', 'the', 'sky', 'is', 'blue'],\n",
       "      dtype='<U7')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import  word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "target_text = 'This is an example showing the stop words filtering. Hello Mr. Smith! How are you doing today? The whether is great and the Python is awesome. The sky is blue '\n",
    "\n",
    "words= np.array(word_tokenize(target_text))\n",
    "ps= PorterStemmer()\n",
    "v_stem= np.vectorize(ps.stem)\n",
    "stemmed_words= v_stem(words)\n",
    "\n",
    "print (target_text)\n",
    "stemmed_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['go', ',', 'went', ',', 'gone', ',', 'go', ',', 'gon', 'na', ',',\n",
       "       'goe', ',', 'go'], dtype='<U4')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one more sample\n",
    "target_text = 'go, went, gone, going, gonna, goes, goings'\n",
    "words= np.array(word_tokenize(target_text))\n",
    "v_stem(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['appli', ',', 'appli', ',', 'appli', ',', 'appli', ',', 'appli'],\n",
       "      dtype='<U5')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# another sample ) \n",
    "target_text = 'apply, applied, applies, applying , applyings'\n",
    "words= np.array(word_tokenize(target_text))\n",
    "v_stem(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### 2.3. Lemmatizing\n",
    "\n",
    "</font>\n",
    "\n",
    "WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Without specified POS  \n",
    "\n",
    "</font>\n",
    "noun by default "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/roma/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "# nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wn_lemmatizer= WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['apply', ',', 'applied', ',', 'applies', ',', 'applying', ',', 'applyings']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[wn_lemmatizer.lemmatize(w) for w in word_tokenize(target_text)] # v - means verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is', ',', 'am', ',', 'are', ',', 'wa', ',', 'were', ',', 'been', ',', 'being']\n"
     ]
    }
   ],
   "source": [
    "# sampl w/o pos tag \n",
    "target_text = 'is, am, are, was, were, been, being'\n",
    "print([wn_lemmatizer.lemmatize(w) for w in word_tokenize(target_text)] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### With specified POS  \n",
    "\n",
    "</font>\n",
    "requesting lemmatizing to verb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is, am, are, was, were, been, being\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['be', ',', 'be', ',', 'be', ',', 'be', ',', 'be', ',', 'be', ',', 'be']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (target_text)\n",
    "[wn_lemmatizer.lemmatize(w,'v') for w in word_tokenize(target_text)] # v - means verb # the same as the following:\n",
    "# [wn_lemmatizer.lemmatize(w,wn.VERB) for w in word_tokenize(target_text)] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### With specified POS  \n",
    "\n",
    "</font>\n",
    "requesting lemmatizing to noun "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['car', ',', 'car']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_text = 'car, cars'\n",
    "[wn_lemmatizer.lemmatize(w,wn.NOUN) for w in word_tokenize(target_text)] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### 2.4. Part of Speech (POS)\n",
    "\n",
    "</font>\n",
    "\n",
    "pos_tag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('state_union')\n",
    "from nltk.corpus import state_union\n",
    "train_text= state_union.raw('2005-GWBush.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRESIDENT GEORGE W. BUSH'S ADDRESS BEFORE A JOINT SESSION OF THE CONGRESS ON THE STATE OF THE UNION\n",
      " \n",
      "February 2, 2005\n",
      "\n",
      "\n",
      "9:10 P.M. EST \n",
      "\n",
      "THE PRESIDENT: Mr. Speaker, Vice President Cheney, members of Congress, fellow citizens: \n",
      "\n",
      "As a new Congress gathers, all of us in the elected branches of government share a great privilege: We've been placed in office by the votes of the people we serve. And tonight that is a privilege we share with newly-elected leaders of Afghanistan, the Palestinian Territo\n"
     ]
    }
   ],
   "source": [
    "print (train_text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Over the next several months, on issue after issue, let us do what Americans have always done, and build a better world for our children and our grandchildren.'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = sent_tokenize(train_text)\n",
    "s = sentences [10]\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words:\n",
      "['Over', 'the', 'next', 'several', 'months', ',', 'on', 'issue', 'after', 'issue', ',', 'let', 'us', 'do', 'what', 'Americans', 'have', 'always', 'done', ',', 'and', 'build', 'a', 'better', 'world', 'for', 'our', 'children', 'and', 'our', 'grandchildren', '.']\n",
      "\n",
      "tags:\n",
      "[('Over', 'IN'), ('the', 'DT'), ('next', 'JJ'), ('several', 'JJ'), ('months', 'NNS'), (',', ','), ('on', 'IN'), ('issue', 'NN'), ('after', 'IN'), ('issue', 'NN'), (',', ','), ('let', 'VB'), ('us', 'PRP'), ('do', 'VB'), ('what', 'WP'), ('Americans', 'NNPS'), ('have', 'VBP'), ('always', 'RB'), ('done', 'VBN'), (',', ','), ('and', 'CC'), ('build', 'VB'), ('a', 'DT'), ('better', 'JJR'), ('world', 'NN'), ('for', 'IN'), ('our', 'PRP$'), ('children', 'NNS'), ('and', 'CC'), ('our', 'PRP$'), ('grandchildren', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# nltk.download('averaged_perceptron_tagger')\n",
    "words= word_tokenize(s)\n",
    "print ('words:\\n{}'.format(words))\n",
    "tagged= nltk.pos_tag(words)\n",
    "print ('\\ntags:\\n{}'.format (tagged))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Review NLTK POS\n",
    "\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n"
     ]
    }
   ],
   "source": [
    "# nltk.download('tagsets')\n",
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Ambiguity of NLTK POS tagging \n",
    "\n",
    "</font>\n",
    "NLTK tags the only one the most probable of possible cases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('visiting', 'VBG'),\n",
       " ('friends', 'NNS'),\n",
       " ('can', 'MD'),\n",
       " ('be', 'VB'),\n",
       " ('surprising', 'JJ')]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'visiting friends can be surprising'\n",
    "tagged = nltk.pos_tag(nltk.word_tokenize(s)) # alternative possible case [('visiting', 'JJ'),...]\n",
    "tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Converting POS formats \n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'visiting': 'v', 'friends': 'n', 'can': '', 'be': 'v', 'surprising': 'a'}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def penn_to_wn(tag):\n",
    "    return get_wordnet_pos(tag)\n",
    "\n",
    "\n",
    "{t[0]: penn_to_wn(t[1]) for t in tagged }\n",
    "\n",
    "# Part-of-speech constants\n",
    "#  ADJ, ADJ_SAT, ADV, NOUN, VERB = 'a', 's', 'r', 'n', 'v'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mention about \n",
    "# 1) pos used in recognizing location \n",
    "# 2) pos used in language model to predict placeholders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Chunks of POS\n",
    "\n",
    "</font>\n",
    "\n",
    "RegexpParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "train_text= state_union.raw('2005-GWBush.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "sentence= America's economy is the fastest growing of any major industrialized nation.\n",
      "\n",
      "tagged:\n",
      "[('America', 'NNP'), (\"'s\", 'POS'), ('economy', 'NN'), ('is', 'VBZ'), ('the', 'DT'), ('fastest', 'JJS'), ('growing', 'NN'), ('of', 'IN'), ('any', 'DT'), ('major', 'JJ'), ('industrialized', 'VBN'), ('nation', 'NN'), ('.', '.')]\n",
      "\n",
      "chunked:\n",
      "<bound method Tree.flatten of Tree('S', [('America', 'NNP'), (\"'s\", 'POS'), ('economy', 'NN'), ('is', 'VBZ'), ('the', 'DT'), Tree('chunk', [('fastest', 'JJS'), ('growing', 'NN')]), ('of', 'IN'), ('any', 'DT'), ('major', 'JJ'), ('industrialized', 'VBN'), ('nation', 'NN'), ('.', '.')])>\n",
      "\n",
      "Detected chunks:\n",
      "['fastest growing']\n",
      "\n",
      "sentence= In the past four years, we provided tax relief to every person who pays income taxes, overcome a recession, opened up new markets abroad, prosecuted corporate criminals, raised homeownership to its highest level in history, and in the last year alone, the United States has added 2.3 million new jobs.\n",
      "\n",
      "tagged:\n",
      "[('In', 'IN'), ('the', 'DT'), ('past', 'JJ'), ('four', 'CD'), ('years', 'NNS'), (',', ','), ('we', 'PRP'), ('provided', 'VBD'), ('tax', 'NN'), ('relief', 'NN'), ('to', 'TO'), ('every', 'DT'), ('person', 'NN'), ('who', 'WP'), ('pays', 'VBZ'), ('income', 'NN'), ('taxes', 'NNS'), (',', ','), ('overcome', 'VBP'), ('a', 'DT'), ('recession', 'NN'), (',', ','), ('opened', 'VBD'), ('up', 'RP'), ('new', 'JJ'), ('markets', 'NNS'), ('abroad', 'RB'), (',', ','), ('prosecuted', 'JJ'), ('corporate', 'JJ'), ('criminals', 'NNS'), (',', ','), ('raised', 'VBD'), ('homeownership', 'NN'), ('to', 'TO'), ('its', 'PRP$'), ('highest', 'JJS'), ('level', 'NN'), ('in', 'IN'), ('history', 'NN'), (',', ','), ('and', 'CC'), ('in', 'IN'), ('the', 'DT'), ('last', 'JJ'), ('year', 'NN'), ('alone', 'RB'), (',', ','), ('the', 'DT'), ('United', 'NNP'), ('States', 'NNPS'), ('has', 'VBZ'), ('added', 'VBN'), ('2.3', 'CD'), ('million', 'CD'), ('new', 'JJ'), ('jobs', 'NNS'), ('.', '.')]\n",
      "\n",
      "chunked:\n",
      "<bound method Tree.flatten of Tree('S', [('In', 'IN'), ('the', 'DT'), ('past', 'JJ'), ('four', 'CD'), ('years', 'NNS'), (',', ','), ('we', 'PRP'), ('provided', 'VBD'), ('tax', 'NN'), ('relief', 'NN'), ('to', 'TO'), ('every', 'DT'), ('person', 'NN'), ('who', 'WP'), ('pays', 'VBZ'), ('income', 'NN'), ('taxes', 'NNS'), (',', ','), ('overcome', 'VBP'), ('a', 'DT'), ('recession', 'NN'), (',', ','), ('opened', 'VBD'), ('up', 'RP'), Tree('chunk', [('new', 'JJ'), ('markets', 'NNS')]), ('abroad', 'RB'), (',', ','), Tree('chunk', [('prosecuted', 'JJ'), ('corporate', 'JJ'), ('criminals', 'NNS')]), (',', ','), ('raised', 'VBD'), ('homeownership', 'NN'), ('to', 'TO'), ('its', 'PRP$'), Tree('chunk', [('highest', 'JJS'), ('level', 'NN')]), ('in', 'IN'), ('history', 'NN'), (',', ','), ('and', 'CC'), ('in', 'IN'), ('the', 'DT'), Tree('chunk', [('last', 'JJ'), ('year', 'NN')]), ('alone', 'RB'), (',', ','), ('the', 'DT'), ('United', 'NNP'), ('States', 'NNPS'), ('has', 'VBZ'), ('added', 'VBN'), ('2.3', 'CD'), ('million', 'CD'), Tree('chunk', [('new', 'JJ'), ('jobs', 'NNS')]), ('.', '.')])>\n",
      "\n",
      "Detected chunks:\n",
      "['new markets', 'prosecuted corporate criminals', 'highest level', 'last year', 'new jobs']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentences = sent_tokenize(train_text)\n",
    "\n",
    "for s in sentences[13:15]:\n",
    "    print('\\nsentence= {}'.format(s))\n",
    "    words= word_tokenize(s)\n",
    "    # print ('words= {}'.format(words))\n",
    "    tagged= nltk.pos_tag(words) # part of speech (POS)  # works for list of words\n",
    "    print('\\ntagged:\\n{}'.format(tagged))\n",
    "\n",
    "    # detect chunks \n",
    "    chunk_gram = 'chunk: {<JJ.?>+<NN.?>+} ' # selects adj and noun followed by\n",
    "    \n",
    "    chunk_parser = nltk.RegexpParser(chunk_gram)\n",
    "    chunked = chunk_parser.parse(tagged)    \n",
    "    print('\\nchunked:\\n{}'.format(chunked.flatten))\n",
    "\n",
    "    # extract chunks \n",
    "    chunks = []\n",
    "    for subtree in chunked.subtrees(filter=lambda t: t.label() == 'chunk'):\n",
    "        chunk = \"\"\n",
    "        for leave in subtree.leaves():\n",
    "            chunk += leave[0] + ' '\n",
    "        chunks.append(chunk.strip())\n",
    "\n",
    "    print('\\nDetected chunks:\\n{}'.format(chunks))\n",
    "    # draw tree \n",
    "    chunked.draw()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Tree.flatten of Tree('PERSON', [('Donald', 'NNP')])>\n",
      "<bound method Tree.flatten of Tree('GPE', [('New', 'NNP'), ('York', 'NNP')])>\n"
     ]
    }
   ],
   "source": [
    "# nltk.download('maxent_ne_chunker')\n",
    "from nltk.tree import Tree\n",
    "s = 'Donald came to New York for summit'\n",
    "words= word_tokenize(s)\n",
    "tagged_words= nltk.pos_tag(words) # part of speech (POS)  # works for list of words\n",
    "chunked= nltk.ne_chunk(tagged_words)\n",
    "# print ('named_entities:\\n{}'.format(chunked))\n",
    "\n",
    "def extract_ne_chunks(chunked): \n",
    "    chunks = []\n",
    "    for i in chunked:\n",
    "        if type(i) == Tree:    \n",
    "            print (i.flatten)\n",
    "            chunks.append(\" \".join([token for token, pos in i.leaves()]))\n",
    "    return chunks\n",
    "\n",
    "\n",
    "extract_ne_chunks(chunked)\n",
    "\n",
    "chunked.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### 2.5. Wordnet\n",
    "\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Synsets\n",
    "\n",
    "</font>\n",
    "\n",
    "wn.synsets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_word = look \n",
      "Synset('expression.n.01')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['expression.n.01',\n",
       " 'look.n.02',\n",
       " 'look.n.03',\n",
       " 'spirit.n.02',\n",
       " 'look.v.01',\n",
       " 'look.v.02',\n",
       " 'look.v.03',\n",
       " 'search.v.02',\n",
       " 'front.v.01',\n",
       " 'attend.v.02',\n",
       " 'look.v.07',\n",
       " 'expect.v.03',\n",
       " 'look.v.09',\n",
       " 'count.v.08']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_word = 'look'\n",
    "print('target_word = {} '.format(target_word))\n",
    "\n",
    "synsets = wn.synsets(target_word)\n",
    "print (synsets[0])\n",
    "[synset.name() for  synset  in synsets]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Lemmas\n",
    "\n",
    "</font>\n",
    "\n",
    "synset.lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expression.n.01:['expression', 'look', 'aspect', 'facial_expression', 'face']\n",
      "look.n.02:['look', 'looking', 'looking_at']\n",
      "look.n.03:['look']\n",
      "spirit.n.02:['spirit', 'tone', 'feel', 'feeling', 'flavor', 'flavour', 'look', 'smell']\n",
      "look.v.01:['look']\n",
      "look.v.02:['look', 'appear', 'seem']\n",
      "look.v.03:['look']\n",
      "search.v.02:['search', 'look']\n",
      "front.v.01:['front', 'look', 'face']\n",
      "attend.v.02:['attend', 'take_care', 'look', 'see']\n",
      "look.v.07:['look']\n",
      "expect.v.03:['expect', 'look', 'await', 'wait']\n",
      "look.v.09:['look']\n",
      "count.v.08:['count', 'bet', 'depend', 'look', 'calculate', 'reckon']\n",
      "\n",
      " Lemma('expression.n.01.aspect')\n"
     ]
    }
   ],
   "source": [
    "for synset in synsets:\n",
    "    print ('{}:{}'.format(synset.name(), [lemma.name() for  lemma in synset.lemmas()]))\n",
    "\n",
    "print ('\\n',synsets[0].lemmas()[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Definitions\n",
    "\n",
    "</font>\n",
    "\n",
    "synset.definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expression.n.01:\tthe feelings expressed on a person's face\n",
      "look.n.02:\tthe act of directing the eyes toward something and perceiving it visually\n",
      "look.n.03:\tphysical appearance\n",
      "spirit.n.02:\tthe general atmosphere of a place or situation and the effect that it has on people\n",
      "look.v.01:\tperceive with attention; direct one's gaze towards\n",
      "look.v.02:\tgive a certain impression or have a certain outward aspect\n",
      "look.v.03:\thave a certain outward or facial expression\n",
      "search.v.02:\tsearch or seek\n",
      "front.v.01:\tbe oriented in a certain direction, often with respect to another reference point; be opposite to\n",
      "attend.v.02:\ttake charge of or deal with\n",
      "look.v.07:\tconvey by one's expression\n",
      "expect.v.03:\tlook forward to the probable occurrence of\n",
      "look.v.09:\taccord in appearance with\n",
      "count.v.08:\thave faith or confidence in\n"
     ]
    }
   ],
   "source": [
    "for synset in synsets:\n",
    "    print ('{}:\\t{}'.format(synset.name(), synset.definition()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Examples\n",
    "\n",
    "</font>\n",
    "synset.examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expression.n.01:\t['a sad expression', 'a look of triumph', 'an angry face']\n",
      "\n",
      "look.n.02:\t['he went out to have a look', 'his look was fixed on her eyes', 'he gave it a good looking at', 'his camera does his looking for him']\n",
      "\n",
      "look.n.03:\t[\"I don't like the looks of this place\"]\n",
      "\n",
      "spirit.n.02:\t['the feel of the city excited him', 'a clergyman improved the tone of the meeting', 'it had the smell of treason']\n",
      "\n",
      "look.v.01:\t['She looked over the expanse of land', 'Look at your child!', 'Look--a deer in the backyard!']\n",
      "\n",
      "look.v.02:\t['She seems to be sleeping', 'This appears to be a very difficult problem', 'This project looks fishy', 'They appeared like people who had not eaten or slept for a long time']\n",
      "\n",
      "look.v.03:\t['How does she look?', 'The child looks unhappy', 'She looked pale after the surgery']\n",
      "\n",
      "search.v.02:\t['We looked all day and finally found the child in the forest', 'Look elsewhere for the perfect gift!']\n",
      "\n",
      "front.v.01:\t['The house looks north', 'My backyard look onto the pond', 'The building faces the park']\n",
      "\n",
      "attend.v.02:\t['Could you see about lunch?', 'I must attend to this matter', 'She took care of this business']\n",
      "\n",
      "look.v.07:\t['She looked her devotion to me']\n",
      "\n",
      "expect.v.03:\t['We were expecting a visit from our relatives', 'She is looking to a promotion', 'he is waiting to be drafted']\n",
      "\n",
      "look.v.09:\t[\"You don't look your age!\"]\n",
      "\n",
      "count.v.08:\t['you can count on me to help you any time', 'Look to your friends for support', 'You can bet on that!', 'Depend on your family in times of crisis']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for synset in synsets:\n",
    "    print ('{}:\\t{}\\n'.format(synset.name(), synset.examples()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Synonyms and Antonyms\n",
    "\n",
    "</font>\n",
    "synset.lemmas()\n",
    "<br>lemma.antonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_word = good \n",
      "\n",
      "synonyms: {'salutary', 'dependable', 'full', 'adept', 'unspoiled', 'ripe', 'trade_good', 'proficient', 'serious', 'soundly', 'just', 'honest', 'safe', 'estimable', 'goodness', 'sound', 'commodity', 'secure', 'effective', 'honorable', 'skillful', 'in_effect', 'respectable', 'unspoilt', 'beneficial', 'undecomposed', 'expert', 'right', 'in_force', 'near', 'practiced', 'well', 'good', 'thoroughly', 'upright', 'dear', 'skilful'}\n",
      "antonyms:\n",
      " [{'evil': 'good'}, {'evilness': 'goodness'}, {'bad': 'good'}, {'badness': 'goodness'}, {'bad': 'good'}, {'evil': 'good'}, {'ill': 'well'}]\n"
     ]
    }
   ],
   "source": [
    "def get_syn_ant(target_word):\n",
    "    synonyms = []\n",
    "    antonyms = []\n",
    "    print('target_word = {} '.format(target_word))\n",
    "\n",
    "    for syn in wn.synsets(target_word):\n",
    "        for l in syn.lemmas(): # use lemmas to obtain synonyms\n",
    "            synonyms.append(l.name())\n",
    "            if l.antonyms():  # some lemmas have antonyms\n",
    "                antonyms.append({la.name(): l.name() for la in l.antonyms()})\n",
    "#                 antonyms.append(l.antonyms()[0].name())  # Assuming selecting just first antonym for lemma\n",
    "    return set(synonyms), antonyms\n",
    "\n",
    "target_word = 'good'\n",
    "synonyms, antonyms =  get_syn_ant (target_word)\n",
    "print ('\\nsynonyms:', synonyms),\n",
    "print ('antonyms:\\n', antonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drink.n.01  vs water.n.01: 0.26666666666666666\n",
      "drink.n.01  vs milk.n.01: 0.25\n",
      "milk.n.01  vs water.n.01: 0.42105263157894735\n",
      "drink.n.01  vs coca_cola.n.01: 0.2222222222222222\n",
      "pepsi.n.01  vs coca_cola.n.01: 0.9090909090909091\n",
      "new_year.n.01  vs santa_claus.n.01: 0.25\n",
      "new_year.n.01  vs christmas.n.01: 0.7142857142857143\n"
     ]
    }
   ],
   "source": [
    "words_pairs= [('drink.n.01', 'water.n.01'),\n",
    "              ('drink.n.01', 'milk.n.01'),\n",
    "              ('milk.n.01', 'water.n.01'),\n",
    "              ('drink.n.01', 'coca_cola.n.01'),\n",
    "              ('pepsi.n.01', 'coca_cola.n.01'),\n",
    "              ('New_year.n.01', 'Santa_Claus.n.01'),\n",
    "              ('New_year.n.01', 'christmas.n.01')\n",
    "              ]\n",
    "for words_pair in words_pairs:\n",
    "    w1 = wn.synset(words_pair[0])\n",
    "    w2 = wn.synset(words_pair[1])\n",
    "    print('{}  vs {}: {}'.format(w1.name(), w2.name(), w1.wup_similarity(w2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### 2.6. Edit Distance\n",
    "\n",
    "</font>\n",
    "\n",
    "nltk.edit_distance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello, hell: 1/1\n",
      "hell, hall: 1/1\n",
      "men, manual: 4/4\n",
      ", : 4/4\n",
      "casual, causal: 1/2\n",
      "top, pot: 2/2\n",
      "top, open: 3/3\n"
     ]
    }
   ],
   "source": [
    "target_pairs= [\n",
    "    ('hello', 'hell'),\n",
    "    ('hell', 'hall'),\n",
    "    ('men', 'manual'),\n",
    "    ('', ''),\n",
    "    ('casual', 'causal'),\n",
    "    ('top', 'pot'),\n",
    "    ('top', 'open')\n",
    "    \n",
    "]\n",
    "for word_1, word_2 in target_pairs:\n",
    "    print ('{}, {}: {}/{}'.format(word_1, word_2, nltk.edit_distance(word_1, word_2,transpositions=True), \n",
    "                                 nltk.edit_distance(word_1, word_2,transpositions=False)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### 2.7. Wordnet similarity\n",
    "\n",
    "</font>\n",
    "synset_1.wup_similarity(synset_1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Wordnet hierarchy\n",
    "\n",
    "</font>\n",
    "WordNet organizes information in a hierarchy \n",
    "Verbs, nouns, adjectives etc. all have separate hierarchy \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src = 'img/wn_tree1.png' height=500 width= 500 align=\"left\">\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src = 'img/wn_tree1.png' height=500 width= 500 align=\"left\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Path similarity\n",
    "\n",
    "</font>\n",
    "\n",
    "$1/(n+1)$ , $n$ - steps between concepts\n",
    "<br>\n",
    "synset_1.path_similarity(synset_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deer and elk: 0.5\n",
      "deer and horse: 0.14285714285714285\n"
     ]
    }
   ],
   "source": [
    "deer_synset = wn.synset('deer.n.01')\n",
    "elk_synset=  wn.synset('elk.n.01')\n",
    "horse_synset=  wn.synset('horse.n.01')\n",
    "print ('{} and {}: {}'.format('deer','elk', deer_synset.path_similarity(elk_synset))) # d= 2 \n",
    "print ('{} and {}: {}'.format('deer','horse', deer_synset.path_similarity(horse_synset))) # d= 7 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Wu-Palmer similarity\n",
    "\n",
    "</font>\n",
    "synset_1.wup_similarity(synset_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deer and elk: 0.967741935483871\n",
      "deer and horse: 0.8\n"
     ]
    }
   ],
   "source": [
    "# comparing to wup_similarity\n",
    "print ('{} and {}: {}'.format('deer','elk', deer_synset.wup_similarity(elk_synset)))  \n",
    "print ('{} and {}: {}'.format('deer','horse', deer_synset.wup_similarity(horse_synset))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drink.n.01 pepsi.n.01 0.2222222222222222\n",
      "drink.n.02 pepsi.n.01 0.18181818181818182\n",
      "beverage.n.01 pepsi.n.01 0.8421052631578947\n",
      "drink.n.04 pepsi.n.01 0.2857142857142857\n",
      "swallow.n.02 pepsi.n.01 0.25\n",
      "drink.v.01 pepsi.n.01 0\n",
      "drink.v.02 pepsi.n.01 0\n",
      "toast.v.02 pepsi.n.01 0\n",
      "drink_in.v.01 pepsi.n.01 0\n",
      "drink.v.05 pepsi.n.01 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8421052631578947"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import editdistance # standalone package\n",
    "\n",
    "def calc_words_similarity(word_a,word_b, verbose= 0):\n",
    "    ''' Calculates wup similarity for all synsets if synsets available, and score based on edit distance otherwise\n",
    "        in case of synsets available returns max of all values\n",
    "    '''\n",
    "    word_a_synsets= wn.synsets(word_a)\n",
    "    word_b_synsets= wn.synsets(word_b)\n",
    "    if len (word_a_synsets)==0 or len (word_b_synsets)==0:\n",
    "        if verbose: \n",
    "            print ('One of words has no synsets')\n",
    "        return calc_edit_score(word_a,word_b)\n",
    "    else:\n",
    "        similarities_all_synsets= []\n",
    "        for word_a_synset in word_a_synsets:\n",
    "            for word_b_synset in word_b_synsets:\n",
    "                if word_a_synset.pos() == word_b_synset.pos():  # this speeds up the calculation                   \n",
    "                    synsets_similarity =  word_a_synset.wup_similarity(word_b_synset)\n",
    "                    if synsets_similarity: # some time return None\n",
    "                        similarities_all_synsets.append(synsets_similarity)\n",
    "                    else: # synsets_similarity is None\n",
    "                        synsets_similarity= 0 \n",
    "                else: # different pos \n",
    "                    synsets_similarity= 0 \n",
    "                similarities_all_synsets.append(synsets_similarity)\n",
    "                if verbose:\n",
    "                    print (word_a_synset.name(), word_b_synset.name(), synsets_similarity)\n",
    "        return max(similarities_all_synsets)\n",
    "\n",
    "def calc_edit_score(w1, w2):\n",
    "    '''calc the score being based on edit (Levinshtein) distance\n",
    "        1 difference - 0.8, 2- 0.64, 3-.51 ...\n",
    "    '''\n",
    "    edit_dist= nltk.edit_distance(w1, w2)    \n",
    "    if edit_dist> len (w1) or edit_dist> len (w2): # no common letter\n",
    "        print ('No common letter. Consider similarity as 0.')\n",
    "        return 0\n",
    "    return 0.8 ** edit_dist\n",
    "\n",
    "calc_words_similarity('drink','pepsi', verbose= 1)\n",
    "# calc_words_similarity('deer','horse', verbose= 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Custom phrase similarity\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Science is fascinating\n",
      "Work with data is interesting\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8634920634920635"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_phrase_similarity(phrase_a, phrase_b):\n",
    "    '''\n",
    "    phrase_a, phrase_b - str\n",
    "    '''\n",
    "    if len (phrase_a) * len (phrase_b)== 0:\n",
    "        return 0\n",
    "\n",
    "    tokens_a = nltk.word_tokenize(phrase_a)\n",
    "    tokens_a = [w for w in tokens_a if w not in stop_words]\n",
    "\n",
    "    tokens_b = nltk.word_tokenize(phrase_b)\n",
    "    tokens_b = [w for w in tokens_b if w not in stop_words]\n",
    "\n",
    "    \n",
    "    a_similarity =  target_phrase_avg(tokens_a, tokens_b)\n",
    "    b_similarity =  target_phrase_avg(tokens_b, tokens_a)\n",
    "\n",
    "    return np.mean([a_similarity, b_similarity])\n",
    "\n",
    "def target_phrase_avg(tokens_a, tokens_b):\n",
    "    all_similarities= [target_word_similarity_score(tokens_a, b) for b in tokens_b]\n",
    "    if len(all_similarities)>0:\n",
    "        return np.mean(all_similarities)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def target_word_similarity_score(current_words, target_word):\n",
    "    '''\n",
    "    Calculates similarity between all current words (all synsets) and ONE target word (all synsets)\n",
    "    :param current_words: list of words of current phrase\n",
    "    :param target_word: str\n",
    "    :return: float\n",
    "    '''\n",
    "    if len(current_words)==0:\n",
    "        return 0\n",
    "    all_current_words_similarities= []\n",
    "    for current_word in current_words:\n",
    "        all_current_words_similarities.append(calc_words_similarity(current_word, target_word))\n",
    "    if len(all_current_words_similarities)==0:\n",
    "        return 0\n",
    "    return max(all_current_words_similarities)\n",
    "\n",
    "phrase_a = 'Data Science is fascinating'\n",
    "phrase_b = 'Work with data is interesting'\n",
    "\n",
    "# phrase_a = 'Data Science is fascinating'\n",
    "# phrase_b = 'the milk is white'\n",
    "\n",
    "print ('{}\\n{}'.format(phrase_a, phrase_b))\n",
    "calc_phrase_similarity(phrase_a, phrase_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## Home Task \n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "moby_raw = gutenberg.raw('melville-moby_dick.txt') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Example 1\n",
    "\n",
    "</font>\n",
    "\n",
    "How many tokens (words and punctuation symbols) are in `moby_raw`?\n",
    "<br>*This function should return an integer.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_one():\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    return len(word_tokenize(moby_raw)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check result\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255,018\n"
     ]
    }
   ],
   "source": [
    "print ('{:,}'.format(example_one()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Example 2\n",
    "\n",
    "</font>\n",
    "\n",
    "How many unique tokens (unique words and punctuation) does `moby_raw` have?\n",
    "<br>*This function should return an integer.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_two():    \n",
    "    return len(set(nltk.word_tokenize(moby_raw)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check result\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08138249064771899\n"
     ]
    }
   ],
   "source": [
    "print ('{:,}'.format(example_two()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Example 3\n",
    "\n",
    "</font>\n",
    "\n",
    "After lemmatizing the verbs, how many unique tokens does `moby_raw` have?\n",
    "<br>*This function should return an integer.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def example_three():\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = [lemmatizer.lemmatize(w,'v') for w in nltk.word_tokenize(moby_raw)]\n",
    "    return len(set(lemmatized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check result\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16,899\n"
     ]
    }
   ],
   "source": [
    "print ('{:,}'.format(example_three()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Question 1\n",
    "\n",
    "</font>\n",
    "\n",
    "\n",
    "What is the lexical diversity of the given text input? (i.e. ratio of unique tokens to the total number of tokens)\n",
    "<br>*This function should return a float.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_one():    \n",
    "    return len(set(nltk.word_tokenize(moby_raw))) / len(word_tokenize(moby_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check result\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08138249064771899"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_one()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Expected Output\n",
    "\n",
    "</font>\n",
    "\n",
    "`0.08139566804842562`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Question 2\n",
    "\n",
    "</font>\n",
    "\n",
    "What percentage of tokens is 'whale'or 'Whale'?\n",
    "<br>*This function should return a float.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "moby_frequencies = FreqDist(nltk.word_tokenize(moby_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_two():\n",
    "    whales = moby_frequencies[\"whale\"] + moby_frequencies[\"Whale\"]\n",
    "    return 100 * (whales/float(len(word_tokenize(moby_raw))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check result\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4125199005560392"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_two()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Expected Output\n",
    "\n",
    "</font>\n",
    "\n",
    "`0.4259007251293193`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Expected Output\n",
    "\n",
    "</font>\n",
    "\n",
    "`0.4259007251293193`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Question 3\n",
    "\n",
    "</font>\n",
    "\n",
    "What are the 20 most frequently occurring (unique) tokens in the text? What is their frequency?\n",
    "<br>*This function should return a list of 10 tuples where each tuple is of the form `(token, frequency)`. The list should be sorted in descending order of frequency.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_three():\n",
    "    return moby_frequencies.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check result\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 19204),\n",
       " ('the', 13715),\n",
       " ('.', 7308),\n",
       " ('of', 6513),\n",
       " ('and', 6010),\n",
       " ('a', 4545),\n",
       " ('to', 4515),\n",
       " (';', 4173),\n",
       " ('in', 3908),\n",
       " ('that', 2978),\n",
       " ('his', 2459),\n",
       " ('it', 2196),\n",
       " ('I', 2111),\n",
       " ('!', 1767),\n",
       " ('is', 1722),\n",
       " ('--', 1713),\n",
       " ('with', 1659),\n",
       " ('he', 1658),\n",
       " ('was', 1639),\n",
       " ('as', 1620)]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_three()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Expected Output\n",
    "\n",
    "</font>\n",
    "\n",
    "`[(',', 19204),\n",
    " ('the', 13715),\n",
    " ('.', 7308),\n",
    " ('of', 6513),\n",
    " ('and', 6010),\n",
    " ('a', 4545),\n",
    " ('to', 4515),\n",
    " (';', 4173),\n",
    " ('in', 3908),\n",
    " ('that', 2978)]`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Question 4\n",
    "\n",
    "</font>\n",
    "\n",
    "What tokens have a length of greater than 5 and frequency of more than 150?\n",
    "<br>*This function should return a sorted list of the tokens that match the above constraints. To sort your list, use `sorted()`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "moby_frequency_frame = pandas.DataFrame(moby_frequencies.most_common(),\n",
    "                                        columns=[\"token\", \"frequency\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>,</td>\n",
       "      <td>19204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the</td>\n",
       "      <td>13715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>.</td>\n",
       "      <td>7308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>of</td>\n",
       "      <td>6513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>and</td>\n",
       "      <td>6010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>a</td>\n",
       "      <td>4545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>to</td>\n",
       "      <td>4515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>;</td>\n",
       "      <td>4173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>in</td>\n",
       "      <td>3908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>that</td>\n",
       "      <td>2978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>his</td>\n",
       "      <td>2459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>it</td>\n",
       "      <td>2196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>I</td>\n",
       "      <td>2111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>!</td>\n",
       "      <td>1767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>is</td>\n",
       "      <td>1722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>--</td>\n",
       "      <td>1713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>with</td>\n",
       "      <td>1659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>he</td>\n",
       "      <td>1658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>was</td>\n",
       "      <td>1639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>as</td>\n",
       "      <td>1620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>''</td>\n",
       "      <td>1615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>'s</td>\n",
       "      <td>1585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>``</td>\n",
       "      <td>1456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>all</td>\n",
       "      <td>1444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>for</td>\n",
       "      <td>1413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>this</td>\n",
       "      <td>1280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>at</td>\n",
       "      <td>1230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>not</td>\n",
       "      <td>1170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>by</td>\n",
       "      <td>1135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>but</td>\n",
       "      <td>1110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20724</th>\n",
       "      <td>intermixingly</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20725</th>\n",
       "      <td>ironical</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20726</th>\n",
       "      <td>coincidings</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20727</th>\n",
       "      <td>destroying</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20728</th>\n",
       "      <td>backwardly</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20729</th>\n",
       "      <td>sky-hawk</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20730</th>\n",
       "      <td>tauntingly</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20731</th>\n",
       "      <td>incommoding</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20732</th>\n",
       "      <td>intercept</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20733</th>\n",
       "      <td>etherial</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20734</th>\n",
       "      <td>thrill</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20735</th>\n",
       "      <td>death-gasp</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20736</th>\n",
       "      <td>Epilogue</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20737</th>\n",
       "      <td>ONLY</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20738</th>\n",
       "      <td>ESCAPED</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20739</th>\n",
       "      <td>THEE</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20740</th>\n",
       "      <td>halfspent</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20741</th>\n",
       "      <td>suction</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20742</th>\n",
       "      <td>closing</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20743</th>\n",
       "      <td>button-like</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20744</th>\n",
       "      <td>Ixion</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20745</th>\n",
       "      <td>Till</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20746</th>\n",
       "      <td>liberated</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20747</th>\n",
       "      <td>Buoyed</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20748</th>\n",
       "      <td>dirgelike</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20749</th>\n",
       "      <td>padlocks</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20750</th>\n",
       "      <td>sheathed</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20751</th>\n",
       "      <td>devious-cruising</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20752</th>\n",
       "      <td>retracing</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20753</th>\n",
       "      <td>orphan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20754 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  token  frequency\n",
       "0                     ,      19204\n",
       "1                   the      13715\n",
       "2                     .       7308\n",
       "3                    of       6513\n",
       "4                   and       6010\n",
       "5                     a       4545\n",
       "6                    to       4515\n",
       "7                     ;       4173\n",
       "8                    in       3908\n",
       "9                  that       2978\n",
       "10                  his       2459\n",
       "11                   it       2196\n",
       "12                    I       2111\n",
       "13                    !       1767\n",
       "14                   is       1722\n",
       "15                   --       1713\n",
       "16                 with       1659\n",
       "17                   he       1658\n",
       "18                  was       1639\n",
       "19                   as       1620\n",
       "20                   ''       1615\n",
       "21                   's       1585\n",
       "22                   ``       1456\n",
       "23                  all       1444\n",
       "24                  for       1413\n",
       "25                 this       1280\n",
       "26                   at       1230\n",
       "27                  not       1170\n",
       "28                   by       1135\n",
       "29                  but       1110\n",
       "...                 ...        ...\n",
       "20724     intermixingly          1\n",
       "20725          ironical          1\n",
       "20726       coincidings          1\n",
       "20727        destroying          1\n",
       "20728        backwardly          1\n",
       "20729          sky-hawk          1\n",
       "20730        tauntingly          1\n",
       "20731       incommoding          1\n",
       "20732         intercept          1\n",
       "20733          etherial          1\n",
       "20734            thrill          1\n",
       "20735        death-gasp          1\n",
       "20736          Epilogue          1\n",
       "20737              ONLY          1\n",
       "20738           ESCAPED          1\n",
       "20739              THEE          1\n",
       "20740         halfspent          1\n",
       "20741           suction          1\n",
       "20742           closing          1\n",
       "20743       button-like          1\n",
       "20744             Ixion          1\n",
       "20745              Till          1\n",
       "20746         liberated          1\n",
       "20747            Buoyed          1\n",
       "20748         dirgelike          1\n",
       "20749          padlocks          1\n",
       "20750          sheathed          1\n",
       "20751  devious-cruising          1\n",
       "20752         retracing          1\n",
       "20753            orphan          1\n",
       "\n",
       "[20754 rows x 2 columns]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moby_frequency_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_four():\n",
    "    frame =  moby_frequency_frame[(moby_frequency_frame.frequency > 150)\n",
    "                                  & (moby_frequency_frame.token.str.len() > 5)]\n",
    "    return sorted(frame.token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check result\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Captain', 'Pequod', 'Queequeg', 'Starbuck', 'almost', 'before', 'himself', 'little', 'seemed', 'should', 'though', 'through', 'whales', 'without']\n"
     ]
    }
   ],
   "source": [
    "print (answer_four())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Expected Output\n",
    "\n",
    "</font>\n",
    "\n",
    "`['Captain', 'Pequod', 'Queequeg', 'Starbuck', 'almost', 'before', 'himself', 'little', 'seemed', 'should', 'though', 'through', 'whales', 'without']`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Question 5\n",
    "\n",
    "</font>\n",
    "\n",
    "Find the longest word in text1 and that word's length.\n",
    "<br>\n",
    "*This function should return a tuple `(longest_word, length)`.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_five():\n",
    "    length = max(moby_frequency_frame.token.str.len())\n",
    "    longest = moby_frequency_frame.token.str.extractall(\"(?P<long>.{{{}}})\".format(length))\n",
    "    return (longest.long.iloc[0], length) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check result\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"twelve-o'clock-at-night\", 23)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_five()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Expected Output\n",
    "\n",
    "</font>\n",
    "\n",
    "`(\"twelve-o'clock-at-night\", 23)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Question 6\n",
    "\n",
    "</font>\n",
    "\n",
    "What unique words have a frequency of more than 2000? What is their frequency?\n",
    "<br>*This function should return a list of tuples of the form `(frequency, word)` sorted in descending order of frequency.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "moby_words = moby_frequency_frame[moby_frequency_frame.token.str.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_six():\n",
    "    common = moby_words[moby_words.frequency > 2000]\n",
    "    return list(zip(common.frequency, common.token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check result\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(13715, 'the'), (6513, 'of'), (6010, 'and'), (4545, 'a'), (4515, 'to'), (3908, 'in'), (2978, 'that'), (2459, 'his'), (2196, 'it'), (2111, 'I')]\n"
     ]
    }
   ],
   "source": [
    "print(answer_six())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Expected Output\n",
    "\n",
    "</font>\n",
    "\n",
    "`[(13715, 'the'), (6513, 'of'), (6010, 'and'), (4545, 'a'), (4515, 'to'), (3908, 'in'), (2978, 'that'), (2459, 'his'), (2196, 'it'), (2097, 'I')]`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Question 7\n",
    "\n",
    "</font>\n",
    "\n",
    "What is the average number of tokens per sentence?\n",
    "<br>*This function should return a float.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "import numpy as np \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_seven():\n",
    "    sentences = sent_tokenize(moby_raw)\n",
    "    counts = (len(nltk.word_tokenize(sentence)) for sentence in sentences)\n",
    "    return sum(counts)/float(len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check result\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.88489646772229\n"
     ]
    }
   ],
   "source": [
    "print(answer_seven())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Expected Output\n",
    "\n",
    "</font>\n",
    "\n",
    "`25.881952902963864`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Question 8\n",
    "\n",
    "</font>\n",
    "\n",
    "What are the 5 most frequent parts of speech in this text? What is their frequency?\n",
    "<br>*This function should return a list of tuples of the form `(part_of_speech, frequency)` sorted in descending order of frequency.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_eight():\n",
    "    tags = nltk.pos_tag(moby_words.token)\n",
    "    frequencies = FreqDist([tag for (word, tag) in tags])\n",
    "    return frequencies.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check result\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NN', 3946), ('JJ', 2958), ('NNP', 2950), ('NNS', 2419), ('VBG', 1402)]\n"
     ]
    }
   ],
   "source": [
    "print(answer_eight())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Expected Output\n",
    "\n",
    "</font>\n",
    "\n",
    "`[('NN', 32730), ('IN', 28657), ('DT', 25867), (',', 19204), ('JJ', 17620)]`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Question 9\n",
    "\n",
    "</font>\n",
    "\n",
    "Create spelling recommender, that take a list of misspelled words and recommends a correctly spelled word for every word in the list.\n",
    "\n",
    "For every misspelled word, the recommender should find find the word in `correct_spellings` that has the shortest `edit distance` (you may need  to use `nltk.edit_distance(word_1, word_2, transpositions=True)`), and starts with the same letter as the misspelled word, and return that word as a recommendation.\n",
    "\n",
    "Recommender should provide recommendations for the three words: `['cormulent', 'incendenece', 'validrate']`.\n",
    "<br>*This function should return a list of length three:\n",
    "`['cormulent_reccomendation', 'incendenece_reccomendation', 'validrate_reccomendation']`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import words\n",
    "from nltk.metrics.distance import (\n",
    "    edit_distance,\n",
    "    jaccard_distance,\n",
    "    )\n",
    "from nltk.util import ngrams\n",
    "correct_spellings = words.words()\n",
    "spellings_series = pandas.Series(correct_spellings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_dist(entries, gram_number):\n",
    "    outcomes = []\n",
    "    for entry in entries:\n",
    "        spellings = spellings_series[spellings_series.str.startswith(entry[0])]\n",
    "        distances = ((jaccard_distance(set(ngrams(entry, gram_number)),\n",
    "                                       set(ngrams(word, gram_number))), word)\n",
    "                     for word in spellings)\n",
    "        closest = min(distances)\n",
    "        outcomes.append(closest[1])\n",
    "    return outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_nine(default_words= ['cormulent', 'incendenece', 'validrate']):\n",
    "    correct_spellings = words.words()\n",
    "    return func_dist(default_words, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Check result\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['corpulent', 'indecence', 'validate']"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_nine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = blue >\n",
    "\n",
    "### Expected Output\n",
    "\n",
    "</font>\n",
    "\n",
    "`['corpulent', 'intendence', 'validate']`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## Learn more\n",
    "</font>\n",
    "\n",
    "NLTK 3.4 documentation\n",
    "<br>\n",
    "https://www.nltk.org/\n",
    "\n",
    "Accessing Text Corpora and Lexical Resources\n",
    "<br>\n",
    "https://www.nltk.org/book/ch02.html\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "nltk.chunk package\n",
    "<br>\n",
    "https://www.nltk.org/api/nltk.chunk.html\n",
    "\n",
    "Edit distance\n",
    "<br>\n",
    "https://en.wikipedia.org/wiki/Edit_distance\n",
    "\n",
    "Applied Text Mining in Python\n",
    "<br>\n",
    "https://www.coursera.org/learn/python-text-mining/home/welcome\n",
    "\n",
    "Natural Language Processing tutorial\n",
    "<br>\n",
    "https://pythonprogramming.net/tokenizing-words-sentences-nltk-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
